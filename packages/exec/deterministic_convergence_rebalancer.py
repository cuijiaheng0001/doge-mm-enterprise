#!/usr/bin/env python3
"""
Phase 7.4 DCR: ç¡®å®šæ€§æ”¶æ•›å†å¹³è¡¡å™¨  
ä¿è¯æ¯è½®åº“å­˜åå·®é€’å‡è‡³å°‘20%ï¼Œ3è½®å†…è¾¾æˆ50/50Â±5%
"""

import logging
import time
import json
from datetime import datetime
from typing import Dict, Optional, List, Tuple

logger = logging.getLogger(__name__)

class DeterministicConvergenceRebalancer:
    """ç¡®å®šæ€§æ”¶æ•›å†å¹³è¡¡å™¨"""
    
    def __init__(self,
                 target_weight: float = 0.50,
                 tolerance: float = 0.05,
                 convergence_rate: float = 0.8,
                 max_rounds: int = 3,
                 urgency_threshold: float = 0.15):
        """
        åˆå§‹åŒ–DCRå†å¹³è¡¡å™¨
        
        Args:
            target_weight: ç›®æ ‡DOGEæƒé‡ (50%)
            tolerance: å®¹å¿è¯¯å·®èŒƒå›´ (Â±5%)
            convergence_rate: æ¯è½®æ”¶æ•›ç‡é˜ˆå€¼ (<0.8 = 20%é€’å‡)
            max_rounds: æœ€å¤§æ”¶æ•›è½®æ•°
            urgency_threshold: é«˜ç´§æ€¥åº¦é˜ˆå€¼ (>15%åå·®)
        """
        self.target_weight = target_weight
        self.tolerance = tolerance
        self.convergence_rate = convergence_rate
        self.max_rounds = max_rounds
        self.urgency_threshold = urgency_threshold
        
        # å†å¹³è¡¡å†å²è®°å½•
        self.rebalance_history: List[Dict] = []
        
        logger.info(f"ğŸ¯ [Phase7.4-DCR] åˆå§‹åŒ–ç¡®å®šæ€§æ”¶æ•›å†å¹³è¡¡å™¨")
        logger.info(f"   ç›®æ ‡æƒé‡: {target_weight:.1%} Â± {tolerance:.1%}")
        logger.info(f"   æ”¶æ•›è¦æ±‚: æ¯è½®é€’å‡>{(1-convergence_rate)*100:.0f}%ï¼Œ{max_rounds}è½®å†…å®Œæˆ")
        logger.info(f"   ç´§æ€¥é˜ˆå€¼: >{urgency_threshold:.1%}")
    
    def calculate_inventory_error(self, doge_value: float, total_value: float) -> float:
        """
        è®¡ç®—åº“å­˜è¯¯å·®
        
        Args:
            doge_value: DOGEèµ„äº§ä»·å€¼ 
            total_value: æ€»èµ„äº§ä»·å€¼
            
        Returns:
            åº“å­˜è¯¯å·® e = w_target - w_current
        """
        if total_value <= 0:
            logger.warning(f"âš ï¸ [Phase7.4-DCR] æ€»èµ„äº§ä»·å€¼å¼‚å¸¸: {total_value}")
            return 0.0
            
        w_current = doge_value / total_value
        error = self.target_weight - w_current
        
        logger.debug(f"ğŸ“Š [Phase7.4-DCR] åº“å­˜è¯¯å·®è®¡ç®—:")
        logger.debug(f"   DOGEä»·å€¼: {doge_value:.2f} USDT")
        logger.debug(f"   æ€»ä»·å€¼: {total_value:.2f} USDT")
        logger.debug(f"   å½“å‰æƒé‡: {w_current:.4f}")
        logger.debug(f"   åº“å­˜è¯¯å·®: {error:.4f}")
        
        return error
    
    def validate_convergence_trajectory(self, e_old: float, e_new: float) -> bool:
        """
        éªŒè¯æ”¶æ•›è½¨è¿¹ï¼š|e_new| < convergence_rate * |e_old|
        
        Args:
            e_old: ä¸Šè½®åº“å­˜è¯¯å·®
            e_new: æœ¬è½®åº“å­˜è¯¯å·®
            
        Returns:
            True: æ”¶æ•›æ­£å¸¸, False: æ”¶æ•›å¤±è´¥
        """
        if abs(e_old) < 0.001:  # é¿å…é™¤é›¶
            return True
            
        convergence_actual = abs(e_new) / abs(e_old)
        converged = convergence_actual < self.convergence_rate
        
        if converged:
            logger.info(f"âœ… [Phase7.4-DCR] æ”¶æ•›éªŒè¯é€šè¿‡:")
            logger.info(f"   |e_old|={abs(e_old):.4f} â†’ |e_new|={abs(e_new):.4f}")
            logger.info(f"   æ”¶æ•›ç‡: {convergence_actual:.3f} < {self.convergence_rate} âœ“")
        else:
            logger.warning(f"âŒ [Phase7.4-DCR] æ”¶æ•›éªŒè¯å¤±è´¥:")
            logger.warning(f"   |e_old|={abs(e_old):.4f} â†’ |e_new|={abs(e_new):.4f}")
            logger.warning(f"   æ”¶æ•›ç‡: {convergence_actual:.3f} >= {self.convergence_rate} (è¦æ±‚<{self.convergence_rate})")
            
        return converged
        
    def generate_deterministic_rebalance_plan(self, 
                                            error: float,
                                            total_usdt: float, 
                                            doge_price: float) -> Optional[Dict]:
        """
        ç”Ÿæˆç¡®å®šæ€§å†å¹³è¡¡æ–¹æ¡ˆ
        
        Args:
            error: åº“å­˜è¯¯å·®
            total_usdt: æ€»èµ„äº§ä»·å€¼(USDT)
            doge_price: DOGEä»·æ ¼
            
        Returns:
            å†å¹³è¡¡è®¡åˆ’å­—å…¸æˆ–None
        """
        if abs(error) < self.tolerance:
            logger.debug(f"âš–ï¸ [Phase7.4-DCR] è¯¯å·®åœ¨å®¹å¿èŒƒå›´å†…: {error:.4f} < {self.tolerance}")
            return None
            
        # è®¡ç®—å†å¹³è¡¡ä»·å€¼å’Œæ•°é‡
        target_doge_value = total_usdt * self.target_weight
        current_doge_value = total_usdt * (self.target_weight - error)
        rebalance_value = target_doge_value - current_doge_value
        rebalance_quantity = abs(rebalance_value) / doge_price
        
        # åˆ¤æ–­ç´§æ€¥åº¦
        urgency = 'HIGH' if abs(error) > self.urgency_threshold else 'NORMAL'
        
        if error > self.tolerance:  # DOGEä¸è¶³ï¼Œéœ€è¦ä¹°å…¥
            plan = {
                'action': 'BUY_DOGE',
                'side': 'BUY', 
                'value': rebalance_value,
                'quantity': rebalance_quantity,
                'urgency': urgency,
                'price_reference': doge_price,
                'error_before': error,
                'expected_error_after': 0.0,  # ç†è®ºä¸Šåº”è¯¥è¾¾åˆ°å¹³è¡¡
                'timestamp': time.time(),
                'datetime': datetime.now().strftime('%H:%M:%S'),
                'convergence_round': len(self.rebalance_history) + 1
            }
            
            logger.info(f"ğŸ“ˆ [Phase7.4-DCR] ç”Ÿæˆä¹°å…¥å†å¹³è¡¡è®¡åˆ’:")
            logger.info(f"   DOGEä¸è¶³: {error:.4f} > {self.tolerance}")
            logger.info(f"   ä¹°å…¥ä»·å€¼: {rebalance_value:.2f} USDT")
            logger.info(f"   ä¹°å…¥æ•°é‡: {rebalance_quantity:.6f} DOGE")
            logger.info(f"   ç´§æ€¥åº¦: {urgency}")
            
        else:  # DOGEè¿‡å¤šï¼Œéœ€è¦å–å‡º
            plan = {
                'action': 'SELL_DOGE',
                'side': 'SELL',
                'value': abs(rebalance_value),
                'quantity': rebalance_quantity, 
                'urgency': urgency,
                'price_reference': doge_price,
                'error_before': error,
                'expected_error_after': 0.0,
                'timestamp': time.time(),
                'datetime': datetime.now().strftime('%H:%M:%S'),
                'convergence_round': len(self.rebalance_history) + 1
            }
            
            logger.info(f"ğŸ“‰ [Phase7.4-DCR] ç”Ÿæˆå–å‡ºå†å¹³è¡¡è®¡åˆ’:")
            logger.info(f"   DOGEè¿‡å¤š: {error:.4f} < {-self.tolerance}")
            logger.info(f"   å–å‡ºä»·å€¼: {abs(rebalance_value):.2f} USDT")
            logger.info(f"   å–å‡ºæ•°é‡: {rebalance_quantity:.6f} DOGE")
            logger.info(f"   ç´§æ€¥åº¦: {urgency}")
        
        return plan
        
    def track_convergence_progress(self, plan: Dict, actual_error_after: float) -> bool:
        """
        è¿½è¸ªæ”¶æ•›è¿›åº¦
        
        Args:
            plan: å·²æ‰§è¡Œçš„å†å¹³è¡¡è®¡åˆ’
            actual_error_after: æ‰§è¡Œåçš„å®é™…è¯¯å·®
            
        Returns:
            True: æ”¶æ•›è¿›åº¦æ­£å¸¸, False: æ”¶æ•›å¤±è´¥
        """
        # æ›´æ–°è®¡åˆ’çš„å®é™…ç»“æœ
        plan_with_result = plan.copy()
        plan_with_result.update({
            'actual_error_after': actual_error_after,
            'execution_timestamp': time.time(),
            'execution_datetime': datetime.now().strftime('%H:%M:%S')
        })
        
        # éªŒè¯æ”¶æ•›è½¨è¿¹
        convergence_ok = True
        if self.rebalance_history:
            last_plan = self.rebalance_history[-1]
            e_old = abs(last_plan['error_before'])
            e_new = abs(actual_error_after)
            
            convergence_ok = self.validate_convergence_trajectory(e_old, e_new)
        
        # æ·»åŠ åˆ°å†å²è®°å½•
        plan_with_result['convergence_validated'] = convergence_ok
        self.rebalance_history.append(plan_with_result)
        
        # é™åˆ¶å†å²è®°å½•é•¿åº¦ 
        if len(self.rebalance_history) > 50:
            self.rebalance_history.pop(0)
            
        logger.info(f"ğŸ“ [Phase7.4-DCR] è®°å½•å†å¹³è¡¡è½®æ¬¡ #{len(self.rebalance_history)}")
        logger.info(f"   æ‰§è¡Œå‰è¯¯å·®: {plan['error_before']:.4f}")
        logger.info(f"   æ‰§è¡Œåè¯¯å·®: {actual_error_after:.4f}")
        logger.info(f"   æ”¶æ•›éªŒè¯: {'âœ…é€šè¿‡' if convergence_ok else 'âŒå¤±è´¥'}")
        
        return convergence_ok
        
    def check_convergence_completion(self) -> Tuple[bool, Dict]:
        """
        æ£€æŸ¥æ”¶æ•›æ˜¯å¦å®Œæˆ
        
        Returns:
            (æ˜¯å¦å®Œæˆ, å®ŒæˆæŠ¥å‘Š)
        """
        if not self.rebalance_history:
            return False, {'status': 'no_data', 'reason': 'æ²¡æœ‰å†å¹³è¡¡è®°å½•'}
            
        latest = self.rebalance_history[-1]
        latest_error = abs(latest['actual_error_after'])
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å®¹å¿èŒƒå›´å†…
        within_tolerance = latest_error < self.tolerance
        
        # æ£€æŸ¥æ”¶æ•›è½®æ•°
        total_rounds = len(self.rebalance_history)
        within_max_rounds = total_rounds <= self.max_rounds
        
        # æ£€æŸ¥æœ€è¿‘å‡ è½®çš„æ”¶æ•›è½¨è¿¹
        convergence_trajectory_ok = True
        failed_convergence_rounds = 0
        
        for plan in self.rebalance_history[-3:]:  # æ£€æŸ¥æœ€è¿‘3è½®
            if not plan.get('convergence_validated', True):
                failed_convergence_rounds += 1
                convergence_trajectory_ok = False
        
        # ç”Ÿæˆå®ŒæˆæŠ¥å‘Š
        if within_tolerance and convergence_trajectory_ok:
            status = 'COMPLETED'
            reason = f'å·²è¾¾æˆç›®æ ‡: è¯¯å·®{latest_error:.4f} < {self.tolerance}, {total_rounds}è½®å†…å®Œæˆ'
        elif not within_tolerance and total_rounds >= self.max_rounds:
            status = 'FAILED_MAX_ROUNDS'
            reason = f'è¶…è¿‡æœ€å¤§è½®æ•°: {total_rounds} >= {self.max_rounds}, è¯¯å·®ä»ä¸º{latest_error:.4f}'
        elif not convergence_trajectory_ok:
            status = 'FAILED_CONVERGENCE'
            reason = f'æ”¶æ•›è½¨è¿¹å¤±è´¥: {failed_convergence_rounds}è½®æœªè¾¾æˆæ”¶æ•›è¦æ±‚'
        else:
            status = 'IN_PROGRESS' 
            reason = f'è¿›è¡Œä¸­: {total_rounds}è½®, å½“å‰è¯¯å·®{latest_error:.4f}'
        
        completion_report = {
            'status': status,
            'reason': reason,
            'total_rounds': total_rounds,
            'latest_error': latest_error,
            'within_tolerance': within_tolerance,
            'within_max_rounds': within_max_rounds,
            'convergence_trajectory_ok': convergence_trajectory_ok,
            'failed_convergence_rounds': failed_convergence_rounds,
            'success_rate': (total_rounds - failed_convergence_rounds) / max(1, total_rounds)
        }
        
        is_completed = status in ['COMPLETED']
        
        if is_completed:
            logger.info(f"ğŸ‰ [Phase7.4-DCR] æ”¶æ•›å®Œæˆ!")
            logger.info(f"   {reason}")
            logger.info(f"   æˆåŠŸç‡: {completion_report['success_rate']:.1%}")
        elif status.startswith('FAILED'):
            logger.warning(f"âŒ [Phase7.4-DCR] æ”¶æ•›å¤±è´¥!")
            logger.warning(f"   {reason}")
            logger.warning(f"   æˆåŠŸç‡: {completion_report['success_rate']:.1%}")
        else:
            logger.info(f"ğŸ”„ [Phase7.4-DCR] æ”¶æ•›è¿›è¡Œä¸­...")
            logger.info(f"   {reason}")
        
        return is_completed, completion_report
        
    def get_rebalance_summary(self) -> Dict:
        """è·å–å†å¹³è¡¡æ‘˜è¦"""
        if not self.rebalance_history:
            return {'status': 'no_data', 'rounds': 0}
            
        completed, report = self.check_convergence_completion()
        latest = self.rebalance_history[-1] 
        
        # ç»Ÿè®¡å„ç±»å†å¹³è¡¡
        buy_plans = [p for p in self.rebalance_history if p['action'] == 'BUY_DOGE']
        sell_plans = [p for p in self.rebalance_history if p['action'] == 'SELL_DOGE']
        high_urgency = [p for p in self.rebalance_history if p['urgency'] == 'HIGH']
        
        summary = {
            'status': report['status'],
            'completed': completed,
            'total_rounds': len(self.rebalance_history),
            'latest_error': latest['actual_error_after'],
            'latest_action': latest['action'],
            'buy_plans': len(buy_plans),
            'sell_plans': len(sell_plans), 
            'high_urgency_plans': len(high_urgency),
            'success_rate': report['success_rate'],
            'completion_report': report
        }
        
        return summary


def create_deterministic_convergence_rebalancer(**kwargs) -> DeterministicConvergenceRebalancer:
    """åˆ›å»ºDCRå®ä¾‹çš„å·¥å‚å‡½æ•°"""
    return DeterministicConvergenceRebalancer(**kwargs)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # æµ‹è¯•DCRç»„ä»¶
    dcr = create_deterministic_convergence_rebalancer()
    
    # æ¨¡æ‹Ÿæ”¶æ•›åœºæ™¯
    test_scenarios = [
        {'doge_val': 300, 'total_val': 1000, 'price': 0.26, 'desc': 'ä¸¥é‡DOGEä¸è¶³(30%)'},
        {'doge_val': 450, 'total_val': 1000, 'price': 0.26, 'desc': 'ä¸­ç­‰DOGEä¸è¶³(45%)'},  
        {'doge_val': 520, 'total_val': 1000, 'price': 0.26, 'desc': 'è½»å¾®DOGEå¹³è¡¡(52%)'}
    ]
    
    for i, scenario in enumerate(test_scenarios):
        print(f"\n{'='*70}")
        print(f"æµ‹è¯•åœºæ™¯ {i+1}: {scenario['desc']}")
        print(f"{'='*70}")
        
        # è®¡ç®—è¯¯å·®
        error = dcr.calculate_inventory_error(scenario['doge_val'], scenario['total_val'])
        
        # ç”Ÿæˆå†å¹³è¡¡è®¡åˆ’
        plan = dcr.generate_deterministic_rebalance_plan(
            error=error,
            total_usdt=scenario['total_val'],
            doge_price=scenario['price']
        )
        
        if plan:
            print(f"å†å¹³è¡¡è®¡åˆ’: {plan['action']}")
            print(f"æ•°é‡: {plan['quantity']:.2f} DOGE")
            print(f"ä»·å€¼: {plan['value']:.2f} USDT")
            print(f"ç´§æ€¥åº¦: {plan['urgency']}")
            
            # æ¨¡æ‹Ÿæ‰§è¡Œç»“æœï¼ˆå‡è®¾æ‰§è¡Œåè¯¯å·®å‡åŠï¼‰
            simulated_error_after = error * 0.5
            convergence_ok = dcr.track_convergence_progress(plan, simulated_error_after)
            
            print(f"æ”¶æ•›éªŒè¯: {'âœ…' if convergence_ok else 'âŒ'}")
        else:
            print("æ— éœ€å†å¹³è¡¡ - è¯¯å·®åœ¨å®¹å¿èŒƒå›´å†…")
            
    # æ£€æŸ¥æœ€ç»ˆæ”¶æ•›çŠ¶æ€
    completed, report = dcr.check_convergence_completion()
    print(f"\n{'='*70}")
    print(f"æœ€ç»ˆæ”¶æ•›æŠ¥å‘Š:")
    print(f"å®ŒæˆçŠ¶æ€: {report['status']}")
    print(f"æ€»è½®æ•°: {report['total_rounds']}")
    print(f"æˆåŠŸç‡: {report['success_rate']:.1%}")
    print(f"åŸå› : {report['reason']}")
    
    summary = dcr.get_rebalance_summary()
    print(f"\nå†å¹³è¡¡æ‘˜è¦: {json.dumps(summary, indent=2, ensure_ascii=False)}")